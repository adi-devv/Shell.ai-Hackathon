### 1. Overall Approach

This solution predicts 10 blend properties (multi-output regression) using a **LightGBM** model wrapped in `sklearn.multioutput.MultiOutputRegressor`. The workflow includes advanced feature engineering, K-Fold cross-validation, and ensembling for robust predictions. The final predictions are saved in `submission.csv`.

**Workflow:**
1. Load data from the `dataset/` directory.
2. Separate features and targets.
3. Perform extensive feature engineering (see below).
4. Align train/test features.
5. Train a `MultiOutputRegressor` with LightGBM using 5-fold cross-validation.
6. Ensemble predictions from all folds.
7. Generate `submission.csv` in the required format.


### 2. Feature Engineering Details

The following feature engineering steps are implemented:

- **Original Features:** All blend composition columns and component property columns are included.
- **Weighted Averages:** For each property, compute the volume-weighted average across all components.
- **Interactions:** Multiply each component's volume by each of its properties to capture interaction effects.
- **Statistical Aggregations:** For each property (across all components), compute min, max, mean, and standard deviation.
- **Differences:** Compute differences between properties of Component 1 and Component 2 for each property.
- **Column Alignment:** Ensure both train and test sets have identical feature columns after engineering, filling missing columns with zeros if needed.

**Note:** The feature engineering is designed to capture both linear and non-linear relationships, as well as potential synergies between blend components.


### 3. Tools Used

- **Python 3.x**
- **Pandas:** Data loading and manipulation
- **NumPy:** Numerical operations
- **LightGBM (`lightgbm`):** Fast, efficient gradient boosting for regression
- **Scikit-learn (`sklearn`):**
  - `MultiOutputRegressor` for multi-target regression
  - `KFold` for cross-validation
  - `mean_absolute_percentage_error` for evaluation
- **Warnings:** Suppress unnecessary output


### 4. Source Files

- `score72.py`: Main script implementing the solution, including feature engineering, model training, validation, and submission file generation.
- `dataset/train.csv`: Training data
- `dataset/test.csv`: Test data for prediction
- `dataset/sample_solution.csv`: Sample submission format
- `submission.csv`: Output file generated by `score72.py` for submission


### 5. Submission Format

The submission file (`submission.csv`) contains:
- `ID`: Test sample identifier
- `BlendProperty1` to `BlendProperty10`: Predicted values for each blend property

The column order matches the sample in `dataset/sample_solution.csv`.


### 6. Notes

- The script prints cross-validation scores and an estimated leaderboard score based on the mean absolute percentage error (MAPE).
- All random seeds and parameters are set for reproducibility, but further tuning may improve results. 