### 1. Overall Approach

This solution predicts 10 blend properties (multi-output regression) using a **target-specific ensemble** of LightGBM and XGBoost models, with hyperparameters tuned via Optuna. The workflow includes advanced feature engineering, per-target feature selection, outlier handling, and blending for optimal performance. The final predictions are saved in `submission_cpu_optimized_mape.csv`.

**Workflow:**
1. Load data from the `dataset/` directory.
2. Clip outliers in features and targets.
3. Apply log transformation to highly skewed targets.
4. Perform advanced feature engineering (see below).
5. Align train/test features.
6. For each target:
   - Select top features using LightGBM importances.
   - Tune LightGBM and XGBoost hyperparameters with Optuna.
   - Train both models with K-Fold cross-validation.
   - Blend predictions using optimized weights.
7. Generate `submission_cpu_optimized_mape.csv` in the required format.


### 2. Feature Engineering Details

The following feature engineering steps are implemented:

- **Original Features:** All blend composition columns and component property columns are included.
- **Weighted Averages:** For each property, compute the volume-weighted average across all components.
- **Selective Interactions:** Multiply each of the first two components' volumes by their properties to capture key interaction effects.
- **Statistical Aggregations:** For each property (across all components), compute the mean.
- **Target-Guided Features:** For each blend volume, bin the values and compute the mean target value per bin, then map this to the feature set.
- **Per-Target Feature Selection:** For each target, select the 50 most important features using LightGBM feature importances.
- **Column Alignment:** Ensure both train and test sets have identical feature columns after engineering, filling missing columns with zeros if needed.

**Note:** Feature engineering is designed to capture both linear and non-linear relationships, as well as potential synergies between blend components and targets.


### 3. Tools Used

- **Python 3.x**
- **Pandas:** Data loading and manipulation
- **NumPy:** Numerical operations
- **LightGBM (`lightgbm`):** Gradient boosting for regression
- **XGBoost (`xgboost`):** Gradient boosting for regression
- **Scikit-learn (`sklearn`):**
  - `KFold` for cross-validation
  - `mean_absolute_percentage_error` for evaluation
  - `MinMaxScaler` for feature scaling
- **Optuna:** Automated hyperparameter optimization
- **Warnings:** Suppress unnecessary output


### 4. Source Files

- `score75.py`: Main script implementing the solution, including feature engineering, model training, validation, and submission file generation.
- `dataset/train.csv`: Training data
- `dataset/test.csv`: Test data for prediction
- `dataset/sample_solution.csv`: Sample submission format
- `submission_cpu_optimized_mape.csv`: Output file generated by `score75.py` for submission


### 5. Submission Format

The submission file (`submission_cpu_optimized_mape.csv`) contains:
- `ID`: Test sample identifier
- `BlendProperty1` to `BlendProperty10`: Predicted values for each blend property

The column order matches the sample in `dataset/sample_solution.csv`.


### 6. Notes

- The script prints per-target and overall cross-validation MAPE scores, as well as an estimated leaderboard score.
- All random seeds and parameters are set for reproducibility, but further tuning may improve results.
- The approach uses target-specific modeling and blending for maximum accuracy. 