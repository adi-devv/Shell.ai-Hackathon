{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmjyP+r4fcIimgsl26hzSm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adi-devv/Shell.ai-Hackathon/blob/main/Fuel_Blending_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJLpQmkWolpw",
        "outputId": "3ef13a85-0d9b-41ac-febe-2175dd87fa72"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/MyDrive/Colab_Projects/Shell_Hackathon/dataset.zip'\n",
        "extract_path = '/content/colab_dataset_unzipped/'\n",
        "\n",
        "!mkdir -p {extract_path}\n",
        "\n",
        "!unzip -o {zip_path} -d {extract_path}\n",
        "\n",
        "print(\"Dataset unzipped successfully!\")\n",
        "\n",
        "!ls {extract_path}\n",
        "!ls {extract_path}/dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AgBlofZowA5",
        "outputId": "e69f1d82-da96-43fb-fea5-ac065c860613"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab_Projects/Shell_Hackathon/dataset.zip\n",
            "   creating: /content/colab_dataset_unzipped/dataset/\n",
            "  inflating: /content/colab_dataset_unzipped/dataset/sample_solution.csv  \n",
            "  inflating: /content/colab_dataset_unzipped/dataset/test.csv  \n",
            "  inflating: /content/colab_dataset_unzipped/dataset/train.csv  \n",
            "Dataset unzipped successfully!\n",
            "dataset\n",
            "sample_solution.csv  test.csv  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_directory = '/content/colab_dataset_unzipped/dataset/' # THIS IS THE CRUCIAL CHANGE\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv(data_directory + 'train.csv')\n",
        "    test_df = pd.read_csv(data_directory + 'test.csv')\n",
        "    sample_submission_df = pd.read_csv(data_directory + 'sample_solution.csv')\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Make sure 'train.csv', 'test.csv', and 'sample_solution.csv' are in the directory: {data_directory}\")\n",
        "    exit()\n",
        "\n",
        "print(\"Train data shape:\", train_df.shape)\n",
        "print(\"Test data shape:\", test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_1wn9yNA_6M",
        "outputId": "25b186ea-dd2a-4080-8691-768e7330b46f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: (2000, 65)\n",
            "Test data shape: (500, 56)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_percentage_error, make_scorer\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.preprocessing import StandardScaler # For potential future use\n",
        "import warnings\n",
        "import random # For setting multiple seeds\n",
        "\n",
        "# Suppress potential warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "# Adjusted path based on your successful unzipping output\n",
        "data_directory = '/content/colab_dataset_unzipped/dataset/' # THIS IS THE CRUCIAL CHANGE\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv(data_directory + 'train.csv')\n",
        "    test_df = pd.read_csv(data_directory + 'test.csv')\n",
        "    sample_submission_df = pd.read_csv(data_directory + 'sample_solution.csv')\n",
        "except FileNotFoundError as e: # Catch the specific error to print it\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    print(f\"Please ensure 'train.csv', 'test.csv', and 'sample_solution.csv' are in the directory: {data_directory}\")\n",
        "    raise # Re-raise the exception to stop execution and show the full traceback\n",
        "\n",
        "\n",
        "print(\"Train data shape:\", train_df.shape)\n",
        "print(\"Test data shape:\", test_df.shape)\n",
        "\n",
        "# --- 2. Define Features and Targets ---\n",
        "target_columns = [f'BlendProperty{i}' for i in range(1, 11)]\n",
        "original_feature_columns = [col for col in train_df.columns if col not in ['ID'] + target_columns]\n",
        "\n",
        "test_ids = test_df['ID']\n",
        "\n",
        "print(\"\\nOriginal Features for training shape (before FE):\", train_df[original_feature_columns].shape)\n",
        "print(\"Targets for training shape:\", train_df[target_columns].shape)\n",
        "print(\"Original Features for testing shape (before FE):\", test_df[original_feature_columns].shape)\n",
        "\n",
        "\n",
        "# --- 3. Enhanced Feature Engineering ---\n",
        "\n",
        "def create_features(df, original_feats):\n",
        "    df_processed = df.drop(columns=['ID'], errors='ignore').copy()\n",
        "\n",
        "    # Start with original features that are relevant (not targets)\n",
        "    df_features = df_processed[original_feats].copy()\n",
        "\n",
        "    blend_cols = [f'Component{i}_vol' for i in range(1, 6)]\n",
        "\n",
        "    # Ensure all blend_cols are float type to avoid issues with division\n",
        "    for col in blend_cols:\n",
        "        if col in df_features.columns:\n",
        "            df_features[col] = df_features[col].astype(float)\n",
        "\n",
        "    # Dictionary to hold all component property columns for easier access\n",
        "    all_prop_cols = [f'Component{c}_Property{p}' for c in range(1, 6) for p in range(1, 11)]\n",
        "\n",
        "    # === CORE FEATURE ENGINEERING ===\n",
        "\n",
        "    # 1. Weighted Averages of Component Properties (Weighted by Volume) - Already good\n",
        "    for prop_num in range(1, 11):\n",
        "        weighted_avg_col = f'BlendProperty{prop_num}_WeightedAvg'\n",
        "        df_features[weighted_avg_col] = 0.0\n",
        "        for comp_num in range(1, 6):\n",
        "            vol_col = f'Component{comp_num}_vol'\n",
        "            prop_col = f'Component{comp_num}_Property{prop_num}'\n",
        "            if vol_col in df_processed.columns and prop_col in df_processed.columns:\n",
        "                # Add check for division by zero if vol_col sum isn't 100 always (though it should be)\n",
        "                df_features[weighted_avg_col] += (df_processed[vol_col] / 100.0) * df_processed[prop_col]\n",
        "\n",
        "\n",
        "    # 2. Interactions between Blend Volume and Component Properties - Already good\n",
        "    for comp_num in range(1, 6):\n",
        "        vol_col = f'Component{comp_num}_vol'\n",
        "        for prop_num in range(1, 11):\n",
        "            prop_col = f'Component{comp_num}_Property{prop_num}'\n",
        "            if vol_col in df_processed.columns and prop_col in df_processed.columns:\n",
        "                df_features[f'{vol_col}_x_{prop_col}'] = df_processed[vol_col] * df_processed[prop_col]\n",
        "\n",
        "    # 3. Statistical Aggregations Across Component Properties - Expanded\n",
        "    for prop_num in range(1, 11):\n",
        "        props_for_agg = [f'Component{comp_num}_Property{prop_num}' for comp_num in range(1, 6)]\n",
        "        existing_props_for_agg = [p for p in props_for_agg if p in df_processed.columns]\n",
        "\n",
        "        if existing_props_for_agg:\n",
        "            df_features[f'Property{prop_num}_min'] = df_processed[existing_props_for_agg].min(axis=1)\n",
        "            df_features[f'Property{prop_num}_max'] = df_processed[existing_props_for_agg].max(axis=1)\n",
        "            df_features[f'Property{prop_num}_mean'] = df_processed[existing_props_for_agg].mean(axis=1)\n",
        "            df_features[f'Property{prop_num}_std'] = df_processed[existing_props_for_agg].std(axis=1).fillna(0)\n",
        "            df_features[f'Property{prop_num}_range'] = df_features[f'Property{prop_num}_max'] - df_features[f'Property{prop_num}_min']\n",
        "\n",
        "            # Count of non-zero components for this property (proxy for active ingredients)\n",
        "            # This indicates how many components contribute to a specific property for a given blend.\n",
        "            df_features[f'Property{prop_num}_active_components_count'] = (df_processed[existing_props_for_agg] != 0).sum(axis=1)\n",
        "\n",
        "\n",
        "    # 4. More Complex Interactions (Interactions between properties of DIFFERENT components)\n",
        "    # This can capture synergy or antagonism between ingredients.\n",
        "    for i in range(1, 6): # Component i\n",
        "        for j in range(i + 1, 6): # Component j (to avoid duplicates and self-interaction)\n",
        "            for p1 in range(1, 11): # Property p1 of Component i\n",
        "                for p2 in range(1, 11): # Property p2 of Component j\n",
        "                    prop_i = f'Component{i}_Property{p1}'\n",
        "                    prop_j = f'Component{j}_Property{p2}'\n",
        "                    if prop_i in df_processed.columns and prop_j in df_processed.columns:\n",
        "                        df_features[f'{prop_i}_x_{prop_j}'] = df_processed[prop_i] * df_processed[prop_j]\n",
        "                        # Add a difference too if deemed relevant for specific properties\n",
        "                        # df_features[f'{prop_i}_minus_{prop_j}'] = df_processed[prop_i] - df_processed[prop_j]\n",
        "\n",
        "\n",
        "    # 5. Ratios (selective ratios if domain knowledge exists, or general ones)\n",
        "    # Be careful with division by zero. Add a small epsilon if denominators can be zero.\n",
        "    epsilon = 1e-6 # A tiny value to prevent division by zero\n",
        "\n",
        "    # Example: Ratio of property 1 between component 1 and component 2\n",
        "    for prop_num in range(1, 11):\n",
        "        comp1_prop = f'Component1_Property{prop_num}'\n",
        "        comp2_prop = f'Component2_Property{prop_num}'\n",
        "        if comp1_prop in df_processed.columns and comp2_prop in df_processed.columns:\n",
        "            df_features[f'Comp1_Prop{prop_num}_div_Comp2_Prop{prop_num}'] = \\\n",
        "                df_processed[comp1_prop] / (df_processed[comp2_prop] + epsilon)\n",
        "            df_features[f'Comp2_Prop{prop_num}_div_Comp1_Prop{prop_num}'] = \\\n",
        "                df_processed[comp2_prop] / (df_processed[comp1_prop] + epsilon)\n",
        "\n",
        "    # 6. Polynomial Features (simple, on volumes or key properties)\n",
        "    # Example: Squares of blend volumes\n",
        "    for vol_col in blend_cols:\n",
        "        if vol_col in df_processed.columns:\n",
        "            df_features[f'{vol_col}_sq'] = df_processed[vol_col] ** 2\n",
        "\n",
        "    # Fill any remaining NaNs created during feature engineering (e.g., from std of single-item groups)\n",
        "    df_features = df_features.fillna(0) # Or consider mean/median imputation\n",
        "\n",
        "    return df_features\n",
        "\n",
        "\n",
        "print(\"\\nPerforming Feature Engineering...\")\n",
        "X_train_fe = create_features(train_df, original_feature_columns)\n",
        "X_test_fe = create_features(test_df, original_feature_columns)\n",
        "\n",
        "# Align columns after feature engineering to ensure consistency\n",
        "train_cols = X_train_fe.columns\n",
        "test_cols = X_test_fe.columns\n",
        "\n",
        "missing_in_test = set(train_cols) - set(test_cols)\n",
        "for c in missing_in_test:\n",
        "    X_test_fe[c] = 0\n",
        "\n",
        "missing_in_train = set(test_cols) - set(train_cols)\n",
        "for c in missing_in_train:\n",
        "    X_train_fe[c] = 0\n",
        "\n",
        "X_test_fe = X_test_fe[train_cols] # Ensure column order is identical\n",
        "\n",
        "\n",
        "y_train = train_df[target_columns]\n",
        "\n",
        "print(\"Features after engineering (X_train_fe) shape:\", X_train_fe.shape)\n",
        "print(\"Features after engineering (X_test_fe) shape:\", X_test_fe.shape)\n",
        "\n",
        "# --- Define KFold here ---\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5-Fold Cross-Validation\n",
        "\n",
        "# --- 4. Model Training (LightGBM with K-Fold Cross-Validation and Hyperparameter Tuning Placeholder) ---\n",
        "\n",
        "# Define a custom MAPE scorer for RandomizedSearchCV\n",
        "def custom_mape_scorer(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    if y_true.ndim == 1:\n",
        "        return -mean_absolute_percentage_error(y_true, y_pred)\n",
        "    else:\n",
        "        return -np.mean([mean_absolute_percentage_error(y_true[:,i], y_pred[:,i]) for i in range(y_true.shape[1])])\n",
        "\n",
        "mape_scorer = make_scorer(custom_mape_scorer, greater_is_better=True)\n",
        "\n",
        "\n",
        "# Base LightGBM regressor for MultiOutputRegressor\n",
        "lgbm = lgb.LGBMRegressor(objective='regression_l1', metric='mae', n_jobs=-1, random_state=42, verbose=-1, device='gpu') # Added device='gpu'\n",
        "\n",
        "# Hyperparameter search space for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'estimator__n_estimators': [1000, 1500, 2000, 2500],\n",
        "    'estimator__learning_rate': [0.005, 0.01, 0.02, 0.03],\n",
        "    'estimator__num_leaves': [31, 64, 128],\n",
        "    'estimator__max_depth': [-1, 10, 15],\n",
        "    'estimator__feature_fraction': [0.7, 0.8, 0.9],\n",
        "    'estimator__bagging_fraction': [0.7, 0.8, 0.9],\n",
        "    'estimator__bagging_freq': [1],\n",
        "    'estimator__lambda_l1': [0, 0.1, 0.5, 1],\n",
        "    'estimator__lambda_l2': [0, 0.1, 0.5, 1],\n",
        "    'estimator__min_child_samples': [20, 30, 40]\n",
        "}\n",
        "\n",
        "# Wrap LGBM in MultiOutputRegressor for tuning\n",
        "multi_output_model_for_tuning = MultiOutputRegressor(lgbm)\n",
        "\n",
        "print(\"\\nStarting Hyperparameter Tuning with RandomizedSearchCV (this may take time)...\")\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=multi_output_model_for_tuning,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring=mape_scorer,\n",
        "    cv=kf,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_fe, y_train)\n",
        "\n",
        "best_lgbm_params = random_search.best_params_\n",
        "print(f\"\\nBest Hyperparameters found: {best_lgbm_params}\")\n",
        "\n",
        "final_lgbm_params = {k.replace('estimator__', ''): v for k, v in best_lgbm_params.items()}\n",
        "\n",
        "# Set up the final model with the best parameters, ensuring GPU is still specified\n",
        "model = MultiOutputRegressor(lgb.LGBMRegressor(**final_lgbm_params, n_jobs=-1, random_state=42, verbose=-1, device='gpu')) # Added device='gpu'\n",
        "\n",
        "# --- 5. Final Model Training and Prediction with K-Fold Cross-Validation ---\n",
        "\n",
        "print(\"\\nStarting Final K-Fold Cross-Validation training with best parameters...\")\n",
        "oof_preds = np.zeros(y_train.shape)\n",
        "test_preds_folds = []\n",
        "fold_mape_scores = []\n",
        "num_seeds = 3\n",
        "seeds = [random.randint(1, 1000) for _ in range(num_seeds)]\n",
        "\n",
        "final_test_predictions_ensemble = []\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\n--- Running K-Fold with seed: {seed} ---\")\n",
        "    current_seed_test_preds = []\n",
        "\n",
        "    # Update model with current seed, ensuring GPU is specified\n",
        "    model = MultiOutputRegressor(lgb.LGBMRegressor(**final_lgbm_params, n_jobs=-1, random_state=seed, verbose=-1, device='gpu')) # Added device='gpu'\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_fe, y_train)):\n",
        "        print(f\"  --- Fold {fold+1}/{kf.n_splits} ---\")\n",
        "        X_train_fold, X_val_fold = X_train_fe.iloc[train_index], X_train_fe.iloc[val_index]\n",
        "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        fold_val_preds = model.predict(X_val_fold)\n",
        "        oof_preds[val_index] = fold_val_preds\n",
        "\n",
        "        fold_test_preds = model.predict(X_test_fe)\n",
        "        current_seed_test_preds.append(fold_test_preds)\n",
        "\n",
        "        fold_mape = []\n",
        "        for i, target_col in enumerate(target_columns):\n",
        "            mape_val = mean_absolute_percentage_error(y_val_fold[target_col], fold_val_preds[:, i])\n",
        "            fold_mape.append(mape_val)\n",
        "        avg_fold_mape = np.mean(fold_mape)\n",
        "        if seed == seeds[0]:\n",
        "             fold_mape_scores.append(avg_fold_mape)\n",
        "        print(f\"  Fold {fold+1} Average MAPE (Seed {seed}): {avg_fold_mape:.4f}\")\n",
        "\n",
        "    final_test_predictions_ensemble.append(np.mean(current_seed_test_preds, axis=0))\n",
        "\n",
        "print(\"\\nFinal Cross-Validation Complete.\")\n",
        "overall_cv_mape = np.mean(fold_mape_scores)\n",
        "print(f\"Overall Cross-Validation Average MAPE (from first seed): {overall_cv_mape:.4f}\")\n",
        "\n",
        "\n",
        "# --- 6. Final Predictions and Submission ---\n",
        "\n",
        "final_submission_predictions = np.mean(final_test_predictions_ensemble, axis=0)\n",
        "\n",
        "predictions_df = pd.DataFrame(final_submission_predictions, columns=target_columns)\n",
        "\n",
        "# --- 7. Generate Submission File ---\n",
        "submission_df = pd.DataFrame({'ID': test_ids})\n",
        "submission_df = pd.concat([submission_df, predictions_df], axis=1)\n",
        "\n",
        "submission_df = submission_df[['ID'] + target_columns]\n",
        "\n",
        "submission_file_name = 'submission.csv'\n",
        "submission_df.to_csv(submission_file_name, index=False)\n",
        "\n",
        "print(f\"\\nSubmission file '{submission_file_name}' created successfully!\")\n",
        "print(\"First 5 rows of the submission file:\")\n",
        "print(submission_df.head())\n",
        "\n",
        "reference_cost_public = 2.72\n",
        "estimated_score_public = 100 * max(0, 1 - overall_cv_mape / reference_cost_public)\n",
        "print(f\"\\nEstimated Public Leaderboard Score (based on first seed's CV MAPE): {estimated_score_public:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqaJI06DCr7O",
        "outputId": "2eda3d94-647e-482c-8899-8d716f683680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: (2000, 65)\n",
            "Test data shape: (500, 56)\n",
            "\n",
            "Original Features for training shape (before FE): (2000, 55)\n",
            "Targets for training shape: (2000, 10)\n",
            "Original Features for testing shape (before FE): (500, 55)\n",
            "\n",
            "Performing Feature Engineering...\n",
            "Features after engineering (X_train_fe) shape: (2000, 1145)\n",
            "Features after engineering (X_test_fe) shape: (500, 1145)\n",
            "\n",
            "Starting Hyperparameter Tuning with RandomizedSearchCV (this may take time)...\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ]
        }
      ]
    }
  ]
}