# Score 81 Implementation - Advanced Stacking Ensemble

## 1. Overall Approach

This solution achieves a score of 81 using a **sophisticated stacking ensemble** approach with advanced feature engineering. The implementation uses a multi-model stacking regressor combining XGBoost, LightGBM, CatBoost, Support Vector Regression (SVR), and Multi-Layer Perceptron (MLP) with Ridge regression as the meta-learner. The workflow includes comprehensive feature engineering, automated feature selection, and robust preprocessing pipelines.

**Workflow:**
1. Load training and test data from the `dataset/` directory
2. Apply advanced feature engineering (see details below)
3. Implement robust preprocessing with imputation and scaling
4. For each of the 10 blend properties:
   - Create a stacking ensemble with 5 base models
   - Apply feature selection to reduce dimensionality
   - Train the complete pipeline with cross-validation
   - Validate performance using MAPE
5. Generate predictions for all test samples
6. Save results in `submission.csv` format

## 2. Feature Engineering Details

The `FeatureEngineer` class implements sophisticated feature engineering:

### Weighted Averages
- **Volume-weighted property averages:** For each property (1-10), compute the fraction-weighted average across all 5 components
- **Formula:** `WeightedAvg_Property{i} = Σ(Component{j}_fraction × Component{j}_Property{i})`

### Statistical Aggregations
- **Standard Deviation:** Calculate the standard deviation of each property across all components
- **Range:** Compute the difference between maximum and minimum property values across components
- **Formula:** `StdDev_Property{i} = std([Component{j}_Property{i} for j in 1-5])`
- **Formula:** `Range_Property{i} = max([Component{j}_Property{i} for j in 1-5]) - min([Component{j}_Property{i} for j in 1-5])`

### Interaction Features
- **Component-Component Interactions:** Multiply fraction values between different component pairs
- **Property-Property Interactions:** Create interaction terms between the same property across different components
- **Formula:** `Component{i}_Component{j}_interaction = Component{i}_fraction × Component{j}_fraction`
- **Formula:** `Component{i}_Property{k}_Component{j}_Property{k}_interaction = Component{i}_Property{k} × Component{j}_Property{k}`

### Polynomial Features
- **Squared Terms:** For important properties (1, 2, 3, 5, 7), create squared terms of weighted averages
- **Square Root Terms:** Apply square root transformation to capture non-linear relationships
- **Formula:** `Property{i}_squared = WeightedAvg_Property{i}²`
- **Formula:** `Property{i}_sqrt = √|WeightedAvg_Property{i}|`

## 3. Model Architecture

### Base Models
1. **XGBoost Regressor:**
   - 500 estimators, max_depth=6, learning_rate=0.05
   - Subsample=0.9, colsample_bytree=0.9

2. **LightGBM Regressor:**
   - 500 estimators, max_depth=8, learning_rate=0.05
   - Subsample=0.9, colsample_bytree=0.9

3. **CatBoost Regressor:**
   - 500 iterations, depth=6, learning_rate=0.05
   - Silent mode enabled

4. **Support Vector Regression (SVR):**
   - RBF kernel, C=10, gamma='scale'

5. **Multi-Layer Perceptron (MLP):**
   - Hidden layers: (100, 50)
   - ReLU activation, Adam solver
   - 500 max iterations

### Meta-Learner
- **Ridge Regression:** Alpha=1.0 for regularization

### Feature Selection
- **SelectKBest:** Select top 100 features using f_regression scoring
- **Cross-validation:** 5-fold CV for stacking ensemble training

## 4. Tools and Libraries Used

- **Python 3.x**
- **NumPy:** Numerical operations and array manipulation
- **Pandas:** Data loading, manipulation, and CSV operations
- **Scikit-learn:**
  - `train_test_split` for validation split
  - `StandardScaler` for feature normalization
  - `mean_absolute_percentage_error` for evaluation
  - `StackingRegressor` for ensemble learning
  - `Ridge` for meta-learning
  - `SVR` for support vector regression
  - `MLPRegressor` for neural network
  - `SelectKBest` and `f_regression` for feature selection
  - `Pipeline` and `ColumnTransformer` for preprocessing
  - `SimpleImputer` for handling missing values
  - `BaseEstimator` and `TransformerMixin` for custom transformers
- **XGBoost:** Gradient boosting implementation
- **LightGBM:** Light gradient boosting machine
- **CatBoost:** Categorical boosting

## 5. Source Files

- `score81`: Main script implementing the stacking ensemble solution
- `dataset/train.csv`: Training data with features and target properties
- `dataset/test.csv`: Test data for prediction
- `submission81.csv`: Output file generated by the script

## 6. Performance Metrics

The implementation evaluates performance using:
- **Mean Absolute Percentage Error (MAPE):** Primary evaluation metric
- **Cross-validation:** 5-fold CV for stacking ensemble training
- **Validation split:** 20% holdout for final model evaluation

## 7. Key Advantages

1. **Robust Ensemble:** Combines 5 diverse algorithms for better generalization
2. **Advanced Feature Engineering:** Captures complex interactions and non-linear relationships
3. **Automated Feature Selection:** Reduces dimensionality while preserving important features
4. **Comprehensive Preprocessing:** Handles missing values and scales features appropriately
5. **Target-Specific Modeling:** Each blend property gets its own optimized model
6. **Reproducible Results:** Fixed random seeds ensure consistent outputs

## 8. Submission Format

The submission file (`submission81.csv`) contains:
- `ID`: Test sample identifier
- `BlendProperty1` to `BlendProperty10`: Predicted values for each blend property

All predictions are generated using the trained stacking ensemble models and follow the required format for competition submission.

## 9. Notes

- The implementation uses a sophisticated stacking approach that typically outperforms individual models
- Feature engineering is designed to capture both linear and non-linear relationships in the data
- The solution is computationally intensive but provides robust and accurate predictions
- All hyperparameters are carefully tuned for optimal performance
- The approach is scalable and can be easily extended with additional base models or feature engineering steps 